
<style>
.collapsible {
  background-color: #f5f5f5;
  color: #444;
  cursor: pointer;
  padding: 18px;
  width: 100%;
  border: none;
  text-align: left;
  outline: none;
  font-size: 15px;
  transition: 0.4s;
  font-weight: bold;
}

.active, .collapsible:hover {
  background-color: #ddd;
}

.collapsible:after {
  content: '\002B';
  color: #777;
  font-weight: bold;
  float: right;
  margin-left: 5px;
}

.active:after {
  content: "\2212";
}

.content {
  padding: 0 18px;
  display: none;
  overflow: hidden;
  background-color: white;
  border: 1px solid #ddd;
}
.div-img{
    justify-content: center;
    align-items: center;
    display:flex;
    padding-top:10%;
    padding-right:10%;
}
img{
    transition: transform .2s;
    width:85%;
    height:85%;
    margin:0 auto;
    background-color: rgb(173, 173, 237);
    border-radius: 10px;
    border: 0px solid black;
}
img:hover{
    transform:scale(1.63);
}
</style>

<button class="collapsible">Projects</button>

<div class="content">
  <p><ul>
  
  <button class="collapsible"><mark style="color:black;font-size:11px;background:#f5f5f5">LEVERAGING TEXT DATA TO EXPLORE HOW NEWS PARTISANSHIP SHAPES DISCOURSE</mark> <mark style="color:gray;font-size:11px;background:#f5f5f5">[NLP; ETL PIPELINE; TRANSFORMER MODELS; BIG TEXT DATA]</mark></button>

<div class="content">
  <p><ul>
 <h2 style="color:Black;font-size:18px;"> Main Contributions</h2>
 

<ul style="text-indent: 20px;"> 
<li> Extracted billions of tweets and replies from Twitter API using SQL, Python, and web scraping techniques </li>
<li> Transformed, cleansed, and normalized with Python, Pandas, and regular expressions </li>
<li> Optimized data collection and transformation tasks using parallel processing, indexing, and caching </li>
<li> Developed custom scripts and automations to streamline data management processes for multiple collaborators  </li>
<li> Ensured data reliability and pipeline stability by developing logging and alerting mechanism to handle errors</li>
</ul>

 <h3 style="color:Black;font-size:18px;"> Summary</h3>

<p style="padding-left: 5px;">In this project, we focused on analyzing the content of Tweets and comments on those tweets. We analyzed broad descriptive patterns in the number of tweets each news outlet makes over time through the course of our data collection, as well as the number of comments, likes, and retweets they receive. Our core analysis involved analyzing the text of the comments and tweets themselves. After scraping, the data was automatically sent to a script that handled tokenization, lemmatization, removing stop words, URLs, numbers and typos, creating n-grams, and vectorization.</p>

<p style="padding-left: 5px;">Once processed, descriptive statistics, such as the average number of words in a comment by news outlet, and the average tone (as measured by sentiment dictionaries, such as LIWC) of comments by news outlet. We compared these descriptive statistics of the comments to the same descriptive statistics of the initial tweets. This allowed us to test hypotheses, such as comments being more emotionally charged than tweets from news outlets. We also examined how the initial source (e.g. news outlet vs. media personality) affected these patterns, expecting that tweets by (and comments on those tweets) media personalities will be more emotionally charged than tweets by (and comments on those tweets) general news outlets.</p> 

<p style="padding-left: 5px;">The main analysis involved using measures of text similarity similar to plagiarism detection to examine the extent to which the comments on a tweet differ from the tweet itself. We are interested in measuring how the conversation through a comment thread shifts the tone and content of the message communicated initially by the news outlets. Dynamic topic modeling will also be used for this purpose.</p>

  
  </ul></p>
</div>


  <button class="collapsible"><mark style="color:black;font-size:11px;background:#f5f5f5">IDENTIFYING WHAT CONSTITUTES A POLITICAL ADVERTISEMENT </mark> <mark style="color:gray;font-size:11px;background:#f5f5f5">
[DATA ANALYSIS & VISUALIZTION; CONJOINT EXPERIMENT]</mark></button>


<div class="content">
  <ul>
  <br>
 <h2 style="color:Black;font-size:18px;"> Main Contributions</h2>
<ul style="text-indent: 20px;">
  <li>Collaborated with multidisciplinary team, providing insights and recommendations based on key findings</li>
<li>Conducted exploratory data analysis using data visualization tools such as ggplot2 and plotly</li>
<li> Identified experiment-breaking distribution error which necessitated reissuance </li>
<li>Implemented data transformation techniques including variable recoding, data aggregation, and normalization</li>
<li>Presented research findings in poster (displayed below) with visualizations, summary and Q & A</li>

</ul>
<ul>
 <div class="div-img">
   <img src='images/poster.jpg'>
 <div></ul>
        
 <h3 style="color:Black;font-size:18px;"> Summary</h3>


<p style="padding-left: 5px;"> In this project, we investigated  social media users' perceptions of digital political ads. We measured users' opinions on how platforms should design political ad UX and policies with the goal of establishing a baseline understanding of user opinions' including the permissibly of political ads and microtargeting, transparency in funding.</p>

<p style="padding-left: 5px;"> The primary objective of this research was to understand what factors of ads (and users themselves) may contribute to their perceptions of how `political' given digital ads are. To do this, we conducted a conjoint experiment asking respondents to compare artificial Facebook ads where we alter their source, content, and political orientation. This conjoint design allows us to isolate the independent effects of each component on perceptions of the political.  </p>

<p style="padding-left: 5px;"> We also conducted a within-between experiment asking respondents to evaluate real ads drawn from the Facebook Ad Library. In this portion of the project, we randomly assigned respondents' to view either a political or non-political advertisement and asked to rate how political they perceived it to be. Respondents rated multiple ads (within-subject variation) but the exact composition of the ads was randomized for each respondent (between-subject variation </p>

<p style="padding-left: 5px;">Overall, our conjoint analysis strongly supports our original research hypotheses showing that the source, strength, and orientation of the message all matter. We found that candidate ads seem to be viewed as inherently political, in contrast to sources such as politically active companies and advocacy organizations, where message strength appears to matter far more in order for an ad to be considered political. This differs from our finding in the conjoint analysis, where ads from companies and advocacy organizations were viewed as equally political.</p>
</div>
<button class="collapsible"><mark style="color:black;font-size:11px;background:#f5f5f5">MEASURING THE EFFICACY OF IN-GROUP CORRECTIONS</mark> <mark style="color:gray;font-size:11px;background:#f5f5f5">[EXPERIMENTAL DESIGN; DATA ANALYSIS & VISUALIZATION]</mark></button>

<div class="content">
  <p><ul>
 <h2 style="color:Black;font-size:18px;"> Main Contributions</h2>
<ul style="text-indent: 20px;">
 <li>Defined research questions and identified target population and sample characteristics</li>
  <li>Developed and organized clear, unbiased questionnaire</li>
  <li>Pilot tested survey instrument and refined based on feedback</li>
  <li>Analyzed data using descriptive and inferential statistics</li>
  <li>Interpreted and reported results with visualizations and tables</li>
</ul>

 <h3 style="color:Black;font-size:18px;"> Summary</h3>

<p style="padding-left: 5px;">In this project we explored the utility of corrections to misinformation on social media. We posit that comments from a member of one's in-group may be more effective in promoting accurate beliefs relative to members of out-groups, especially when the misinformation relates to their ethnic group. We believe individuals will rely on cues such as name and profile picture to evaluate their closeness to one's ethnic group. We suspect that in-group closeness will increase the credibility of a comment relative to a comment from a member of another group because of the sense of cohesion between members of the same ethnic group.</p>


<p style="padding-left: 10px;">Hypothesis 1: Corrective comments will reduce misperceptions (i.e. misperceptions will be lower in the correction conditions than in the no correction condition and control condition)</p>

<p style="padding-left: 10px;">Hypothesis 2: Corrective comments from culturally relevant organizations will more effectively reduce misperceptions</p>

<p style="padding-left: 5px;">To test if corrective comments from in-groups are more effective than comments from members of other groups in promoting accurate beliefs we conducted three experiments. In these experiments, we presented respondents' with a fake Facebook posts containing misinformation targeting specific ethnic groups. We randomly sorted participants into conditions in which they either saw (1) no misinformation post and no comments; (2) misinformation post and no replies (3) misinformation post with in-group correction (4) misinforation post with out-group correction. Before and after the treatment (misinformation post) participants were asked their belief in specific misinformation. </p>

<p style="padding-left: 5px;">The experiments relied on two survey waves that contained an oversampling of black (both surveys) and Latino (first survey only) respondents. Both experiments were sponsored by the Weidenbaum center and distributed by NORC. Study 1 had a total of 2030 respondents. There were ~500 black respondents and ~500 Latino respondents. Study 2 had a total of 1502 which included ~500 black respondents</p>

<p style="padding-left: 5px;">Across the two experiments we found no evidence supporting our theories. Study 1 found that providing corrections in the comments is an effective means to reducing misperceptions, however, the culturally relevant correction was not particularly effective among Latinos. Study 2 did not find results to uphold the theory that culturally relevant comments are more effective at reducing misperceptions. Among all participants in both experiments, the in-group corrections from those who were the target of misinformation was most effective, which may suggest that members of out-groups defer to other ethnic groups when the misinformation does not relate to them</p>


</div>

<button class="collapsible"><mark style="color:black;font-size:11px;background:#f5f5f5">TUNING HYPER-PARAMETERS TO IMPROVE DEEP LEARNING ELECTION PREDICTIONS </mark> <mark style="color:gray;font-size:11px;background:#f5f5f5"> [] </mark></button>

<div class="content">
  <p><ul>
 <h2 style="color:Black;font-size:18px;"> Main Contributions</h2>
<ul style="text-indent: 20px;"> <li> Managed data from three experiments and provided team regular Github updates </li>
<li> Visualized dozens of cross-tabulations between opinions and traits </li>
<li> Identified administrative error which necessitated redistribution </li>
<li> Cleansed and transformed data for linear regression </li>
<li> Visualized data and summarized methodology and results </li>
<li> Presented poster (displayed below) at The Annual Conference for Political Methodology 2022 </li>
</ul>  
  
  </ul></p>
</div>


<div class="content">
  <p><ul>
  
  </ul></p>
</div>
  </ul></p>
</div>

<button class="collapsible">Methods coursework</button>

<div class="content">
  <p><ul>
  <li><strong>Mathematical modeling</strong> (2017)<br>
  <p style="padding-left: 20px;">Taught by Randy Calvert. Primary materials: <i>Mathematics for Economists</i> (Pemberton and Rau). Required.<br>
  This course covered single-variable calculus and portions of multi-variate calculus, linear algebra, and probability theory exposing us to topics including sets and relations; probability; differential calculus and optimization; difference equations; and linear algebra.</li></p>
  
  <li><strong>Research design</strong> (2017)<br>
 <p style="padding-left: 20px;"> Taught by Matt Gabel. Primary materials: <i> Political Science and the Logic of Representations</i> (Kevin A. Clarke, David M. Primo); <i>The logic of real arguments</i> (Alec Fisher). Required.<br>
  This course focused on the philosophy of science and its implications for and applications in political science research. The course had three parts:  examining the nature of scientific knowledge and scientific progress; considering how scientific principles of defining, evaluating, and developing knowledge can be applied to understanding political phenomena; identifying standards for evaluation and making good social scientific arguments and explanations.</li></p>
  
  <li><strong>Quantitative political methodology I</strong> (2018)<br>
  <p style="padding-left: 20px;">Taught by Guillermo Rosas. Primary materials: <i> Linear Models with R </i>(Julian Faraway). Required.<br>
  This course explored the fundamentals of linear regression models in both scalar and matrix form. Problem sets focused on estimation; inference; specification; diagnostic tools; data management; statistical computation.</li>
  
  <li><strong>Theories of Individual and Collective Choice I</strong> (2018)<br>
  <p style="padding-left: 20px;">Taught by Keith Schnakenberg. Primary materials: <i> Game Theory: An Introduction</i> (Steven Tadelis). Required.<br>
  This class was in an introduction to rational choice theory and exposed us to spatial theory of electoral competition; cooperative game theory; and general equilibrium theory.</li>
  
  <li><strong>Applied statistical programming</strong> (2018)<br>
  <p style="padding-left: 20px;">Taught by Jacob Montgomery (advisor). Primary materials: <i> R for dummies</i> (de Vries and Meys); <i>Advanced R</i> (Hadley Wickham).<br>
  This course aimed to build our skill in programming (R) and expose us to the underpinnings of object-oriented programming. Focused on teaching foundational meta-skills from computer science and statistics such as structures; control/flow; functions; version control/documentation; classes and methods; apply/parallel; debugging and creating packages.</li>
  
  <li><strong>Causal inference</strong> (2018)<br>
  <p style="padding-left: 20px;">Taught by Julia Park.<br>
  This coursed aimed to introduce theoretical frameworks for causality and the empirical tools used in the estimation of causal effects. This class had us learn and apply skills related to outcomes; causal graphs; experiments; matching; regression; difference-in-differences; instrumental variables; sensitivity analysis; regression discontinuity etc.</li>
  
  <li><strong>Quantitative political methodology II</strong> (2018)<br>
  <p style="padding-left: 20px;">Taught by Jacob Montgomery (advisor). Primary materials: <i> Linear Models with R</i> (Julian Faraway). Required.<br>
  This is a second course in political methodology covering advanced methods of statistical analysis for political and other social scientists. Covers maximum likelihood estimation for various cross-sectional, time series, and measurement models.</li>
  
  <li><strong>Maximum likelihood estimation</strong> (2019)<br>
  <p style="padding-left: 20px;">Taught by Julia Park.<br>
  This course focused on generalized linear model estimation. We had practical exposure to link functions for a number of models including multinomial and unordered models; ordered outcome models; duration models; count models etc.</li>
  
  <li><strong>Computational social science</strong> (2019)<br>
  <p style="padding-left: 20px;">Taught by Christopher Lucas.<br> Primary materials: <i> Pattern Recognition and Machine Learning</i> (Christopher Bishop);<i> A Course in Machine Learning</i>  (Hal Daum√©); <i>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</i>  (Jerome Friedman, Trevor Hastie, Robert Tibshirani).<br>
This coursed focused on exposing us to different types of data; including networks; text; audio; images; and videos. We began with a mechanistic approaches to supervised and unsupervised learning, then moved to statistical inference with probabilistic interpretations.</ul></p>
</div>



<script>
var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script> 
